{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c567a-96f7-42a6-ae7f-a314ccd5ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713bf956-efef-4c5f-bc83-f8c0f39e3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 50\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using GPU\n",
    "\n",
    "    # Ensure deterministic behavior (slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7662430-c583-4707-a0f8-d9620ce7d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(\"Biraffe2\")\n",
    "suffix = \"_GEQ.csv\"   # example target ending\n",
    "\n",
    "files = [\n",
    "    f for f in folder.iterdir()\n",
    "    if f.is_file()\n",
    "    and f.name.endswith(suffix)\n",
    "    # Files that are of size less than 2KB are empty\n",
    "    and f.stat().st_size > 2 * 1024  # > 2 KB\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c9ab6-4919-4619-87ff-8445f31efe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    data.append(df)\n",
    "    print(f\"Loaded {f}, rows = {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d04f0-fd9c-471c-8709-af94652588ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1000\n",
    "sampling_rate = 1000\n",
    "feature_cols = [\n",
    "    \"ECG\",# \"EDA\",\n",
    "]\n",
    "\n",
    "feature_cols = [\n",
    "    \"EDA\",\n",
    "]\n",
    "\n",
    "label_cols = [\n",
    "    \"Competence13\",\"Immersion13\",\"Flow13\",\"Tension13\",\n",
    "    \"Challenge13\",\"Negative_affect13\",\"Positive_affect13\",\n",
    "    \"Positive_affect18\",\"Negative_affect18\",\n",
    "    \"Competence18\",\"Flow18\",\"Immersion18\",\n",
    "]\n",
    "\n",
    "label_cols = [\n",
    "    #\"Immersion13\",\"Flow13\",\"Tension13\",\n",
    "    # \"Challenge13\",\"Negative_affect13\",\"Positive_affect13\"\n",
    "    \"Negative_affect18\", \"Positive_affect18\",\n",
    "]\n",
    "\n",
    "negativeIndex = 0\n",
    "positiveIndex = 1\n",
    "\n",
    "X_all = [] \n",
    "y_all = []\n",
    "\n",
    "\n",
    "def processDataframe(df, X, y):\n",
    "\n",
    "\n",
    "    for game in df[\"Game_nr\"].unique():\n",
    "        game_df = df[df[\"Game_nr\"] == game]\n",
    "\n",
    "\n",
    "        data_ecg = game_df[\"ECG\"].to_numpy()\n",
    "        data_eda = game_df[\"EDA\"].to_numpy()\n",
    "        label_value = game_df[label_cols].iloc[0]\n",
    "        # print(\"ECG: \", data_ecg)\n",
    "        # break\n",
    "\n",
    "\n",
    "        if label_value.isna().any():\n",
    "            print(f\"Skipping Game {game} in â€” label is NaN\")\n",
    "            continue\n",
    "\n",
    "        label = label_value.to_numpy()\n",
    "            \n",
    "        # print(label)\n",
    "\n",
    "\n",
    "        num_sequences = len(data_ecg) // seq_len\n",
    "\n",
    "        for i in range(2, num_sequences):\n",
    "            start = i * seq_len\n",
    "            end = start + seq_len\n",
    "            seq = np.array([ data_ecg[start:end].reshape(-1), data_eda[start:end].reshape(-1) ]) # np.array(data_eda[start:end].reshape(-1))\n",
    "            # signal = data_ecg[start:end].reshape(-1)\n",
    "            # print(\"Len: \", signal)\n",
    "            # try:\n",
    "            #     signals, info = nk.eda_process(signal, sampling_rate=1000)\n",
    "            # except:\n",
    "            #     continue\n",
    "            # Delineate the ECG signal and visualizing all peaks of ECG complexes\n",
    "            # Extract R-peaks locations\n",
    "            # _, rpeaks = nk.ecg_peaks(signal, sampling_rate=sampling_rate)\n",
    "            # _, waves_peak = nk.ecg_delineate(signal, \n",
    "            #                                  rpeaks, \n",
    "            #                                  sampling_rate=sampling_rate, \n",
    "            #                                  method=\"peak\", \n",
    "            #                                  show=True, \n",
    "            #                                  show_type='peaks')\n",
    "            # print(\"Signal\", signals[\"EDA_Clean\"])\n",
    "            # seq = torch.tensor(waves_peak)#.reshape(-1)\n",
    "            # seq = torch.tensor(signals[\"EDA_Clean\"])#.reshape(-1)\n",
    "\n",
    "            # X.append(seq.reshape(-1, 1)) // Used with positional encoding so we have dim (x, 1)\n",
    "            # print(seq.reshape(-1).shape)\n",
    "            X.append(seq.reshape(-1))\n",
    "            # X.append(seq)\n",
    "            # X.append(waves_peak)\n",
    "            y.append(label)  # label = game nr\n",
    "            # break\n",
    "        # break\n",
    "\n",
    "            \n",
    "for n in range(len(data)):\n",
    "    processDataframe(data[n], X_all, y_all)\n",
    "\n",
    "print(np.array(X_all).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cedf09d-5d7f-430a-b621-5dd221b042b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPoints = []\n",
    "component = \"Negative_affect18\"\n",
    "component = \"Positive_affect18\"\n",
    "np.concatenate(([], [0.75, 3.25]))\n",
    "for pData in data:\n",
    "    # print(pData[\"Immersion18\"].unique())\n",
    "    dataPoints = np.concatenate((dataPoints, pData[component].unique()))\n",
    "pd.array(dataPoints).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2979b-61de-435e-b728-8f2cdddfb9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the array items are tensors and not np arrays\n",
    "# We need to convert back to np arrays for the loader to accept them\n",
    "for n in range(len(X_all)):\n",
    "    X_all[n] = np.array(X_all[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2717282-d058-443d-9094-3c1ddec73795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inputs: \", len(X))\n",
    "print(\"Labels: \", len(y))\n",
    "# Process the raw EDA signal\n",
    "signals, info = nk.eda_process(data[0][\"EDA\"][0:1000], sampling_rate=1000)\n",
    "# print(data[0][\"EDA\"][0:1000])\n",
    "print(signals[\"EDA_Phasic\"])\n",
    "print(data[0][\"EDA\"][0:1000].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408481f-319f-49f5-b1f9-892ed95f4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ecg_train, X_ecg_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=seed, shuffle=True\n",
    ")\n",
    "\n",
    "X_ecg_valid, X_ecg_test, y_valid, y_test = train_test_split(\n",
    "    X_ecg_test, y_test, test_size=0.5, random_state=seed, shuffle=True\n",
    ")\n",
    "\n",
    "# print(X_ecg_train)\n",
    "\n",
    "X_ecg_train = torch.tensor(X_ecg_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_ecg_test = torch.tensor(X_ecg_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "X_ecg_valid = torch.tensor(X_ecg_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32)\n",
    "\n",
    "x_mean = X_ecg_train.mean().item()\n",
    "x_std = X_ecg_train.std().item()\n",
    "\n",
    "y_mean = y_train.mean().item()\n",
    "y_std = y_train.std().item()\n",
    "print(\"y mean:\", y_mean)\n",
    "print(\"y std:\", y_std)\n",
    "print(\"min/max:\", y_train.min().item(), y_train.max().item())\n",
    "\n",
    "print(\"y mean:\", ((y_train - y_mean) / y_std).std())\n",
    "\n",
    "def minMaxMean(data, index = -1):\n",
    "    maxNr = -1e100\n",
    "    minNr = 1e100\n",
    "    mean = 0\n",
    "    for data_x in data:\n",
    "        if index >= 0 or index < len(data_x):\n",
    "            tempMax = data_x[index]\n",
    "            tempMin = data_x[index]\n",
    "            mean += data_x[index] / len(data)\n",
    "        else: \n",
    "            tempMax = data_x.max()\n",
    "            tempMin = data_x.min()\n",
    "            mean += data_x.mean() / len(data)\n",
    "        if tempMax > maxNr:\n",
    "            maxNr = tempMax\n",
    "        if tempMin < minNr:\n",
    "            minNr = tempMin\n",
    "    return minNr, maxNr, mean\n",
    "\n",
    "in_min, in_max, in_mean = minMaxMean(X_ecg_train)\n",
    "n_label_min, n_label_max, n_label_mean = minMaxMean(y_train, index=0)\n",
    "p_label_min, p_label_max, p_label_mean = minMaxMean(y_train, index=1)\n",
    "\n",
    "\n",
    "print(\"In Min: \", in_min)\n",
    "print(\"In Max: \", in_max)\n",
    "print(\"In Mean: \", in_mean)\n",
    "\n",
    "print(\"\")\n",
    "print(\"N Label Min: \", n_label_min)\n",
    "print(\"N Label Max: \", n_label_max)\n",
    "print(\"N Label Mean: \", n_label_mean)\n",
    "\n",
    "print(\"\")\n",
    "print(\"P Label Min: \", p_label_min)\n",
    "print(\"P Label Max: \", p_label_max)\n",
    "print(\"P Label Mean: \", p_label_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7df020-8c95-4fc2-947b-2145c3387dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputLabel(output):\n",
    "    return round(output * 10)\n",
    "n_y_dist = {}\n",
    "p_y_dist = {}\n",
    "\n",
    "for y in y_train:\n",
    "    label = getOutputLabel(y[negativeIndex].item())\n",
    "    n_y_dist[label] = n_y_dist.get(label, 0) + 1\n",
    "    label = getOutputLabel(y[positiveIndex].item())\n",
    "    p_y_dist[label] = p_y_dist.get(label, 0) + 1\n",
    "\n",
    "n_keys_sorted = sorted(n_y_dist.keys())\n",
    "p_keys_sorted = sorted(p_y_dist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e6e52-553a-4231-9e1c-8c8558b6b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrOfBins = 3\n",
    "\n",
    "def splitToGroups(dataDict, nrOfGroups):\n",
    "    split_size = round(len(dataDict.keys()) / nrOfGroups)\n",
    "    dictionary = {}\n",
    "    label = 0\n",
    "    group_count = 0\n",
    "    \n",
    "    for n in range(0, 41):\n",
    "        if dataDict.get(n, -1) != -1:\n",
    "            dictionary[n] = label\n",
    "            group_count += 1\n",
    "            if group_count % split_size == 0 and label < nrOfGroups - 1:\n",
    "                label += 1\n",
    "    return dictionary\n",
    "\n",
    "n_binned_labels = splitToGroups(n_y_dist, nrOfBins)\n",
    "p_binned_labels = splitToGroups(p_y_dist, nrOfBins)\n",
    "\n",
    "print(n_binned_labels)\n",
    "print(p_binned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e0aae-0881-4f9f-925f-c47baf5ec0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_size = [0, 0, 0]\n",
    "p_splits_size = [0, 0, 0]\n",
    "for n in range(0, 42, 2):\n",
    "    if n_y_dist.get(n, -1) != -1:\n",
    "        n_splits_size[n_binned_labels.get(n)] += n_y_dist.get(n, 0)\n",
    "\n",
    "    if p_y_dist.get(n, -1) != -1:\n",
    "        p_splits_size[p_binned_labels.get(n)] += p_y_dist.get(n, 0)\n",
    "print(\"Negative Distribution\", n_splits_size)\n",
    "print(\"Positive Distribution\", p_splits_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b5fa5-6dbf-4f06-b15f-a1da51e52617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valid_binnedLabels))\n",
    "print(len(X_ecg_valid))\n",
    "print(len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eae641-886e-4d6b-b3ed-86865f343cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(\n",
    "    # TensorDataset(((X_ecg_train - x_mean) / x_std), ((y_train - y_mean) / y_std)),\n",
    "    # TensorDataset(X_ecg_train, y_train),\n",
    "    # TensorDataset( X_ecg_train, ((y_train - label_min) / (label_max - label_min)) ),\n",
    "    TensorDataset( X_ecg_train, torch.tensor(binnedLabels, dtype=torch.long) ),\n",
    "    # TensorDataset(X_ecg_train[0], y_train[0]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    # TensorDataset(((X_ecg_train - x_mean) / x_std), ((y_train - y_mean) / y_std)),\n",
    "    # TensorDataset(X_ecg_train, y_train),\n",
    "    # TensorDataset( X_ecg_train, ((y_train - label_min) / (label_max - label_min)) ),\n",
    "    TensorDataset( X_ecg_valid, torch.tensor(valid_binnedLabels, dtype=torch.long) ),\n",
    "    # TensorDataset(X_ecg_train[0], y_train[0]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    # TensorDataset(((X_ecg_train - x_mean) / x_std), ((y_train - y_mean) / y_std)),\n",
    "    # TensorDataset(X_ecg_train, y_train),\n",
    "    # TensorDataset( X_ecg_train, ((y_train - label_min) / (label_max - label_min)) ),\n",
    "    TensorDataset( X_ecg_test, torch.tensor(test_binnedLabels, dtype=torch.long) ),\n",
    "    # TensorDataset(X_ecg_train[0], y_train[0]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87025bc9-2053-47db-86a6-39d0bb4ad12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yTemp = ((y_train - label_min) / (label_max - label_min))\n",
    "summation = [0, 0]\n",
    "for labelTemp in yTemp:\n",
    "    for i in range(len(labelTemp)):\n",
    "        summation[i] += labelTemp[i]\n",
    "summation\n",
    "\n",
    "# Data driven pinning\n",
    "\n",
    "# for mean in summation:\n",
    "#     print(mean / len(yTemp))\n",
    "labelsMean = torch.tensor(np.array(summation) / len(yTemp)).to(device)\n",
    "labelsMean\n",
    "\n",
    "# signals, info = nk.eda_process(X[0], sampling_rate=1000)\n",
    "pinnedLabels = []\n",
    "for n in yTemp:\n",
    "    if n[0].item() < labelsMean[0].item():\n",
    "        pinnedLabels.append(0)\n",
    "    else:\n",
    "        pinnedLabels.append(1)\n",
    "\n",
    "X_ecg_test = torch.tensor(X_ecg_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "validationPinnedLabels = []\n",
    "validationTemp = ((y_test - label_min) / (label_max - label_min))\n",
    "for n in validationTemp:\n",
    "    if n[0].item() < labelsMean[0].item():\n",
    "        validationPinnedLabels.append(0)\n",
    "    else:\n",
    "        validationPinnedLabels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893f708-cec3-4056-9609-ff54cb211774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = 8\n",
    "# print(X_ecg_train[0:size:].shape)\n",
    "# print(y_train[0:size:].shape)\n",
    "# train_loader = DataLoader(\n",
    "#     # TensorDataset(X_ecg_train, ((y_train - y_mean) / y_std)),\n",
    "#     TensorDataset(X_ecg_train[0:size:], y_train[0:size:]),\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     TensorDataset(X_ecg_train, ((y_train - y_mean) / y_std)),\n",
    "#     # TensorDataset(X_ecg_train[0], y_train[0]),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    # TensorDataset(((X_ecg_train - x_mean) / x_std), ((y_train - y_mean) / y_std)),\n",
    "    # TensorDataset( ((X_ecg_train - in_min) / (in_max - in_min)), ((y_train - label_min) / (label_max - label_min)) ),\n",
    "    TensorDataset( X_ecg_train, ((y_train - label_min) / (label_max - label_min)) ),\n",
    "    # TensorDataset(X_ecg_train[0], y_train[0]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97556e2-1081-4f38-80f2-36d362654843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(TensorDataset(torch.stack(X_ecg_train).cuda(), torch.stack(y_train).cuda()), batch_size=64, shuffle=True, num_workers=4)\n",
    "class MeanPenaltyLoss(nn.Module):\n",
    "    def __init__(self, base_loss=\"L2\", penalty_weight=0.1, alpha=1.0):\n",
    "        super().__init__()\n",
    "        if base_loss == \"L1\":\n",
    "            self.base_loss = nn.L1Loss()\n",
    "        elif base_loss == \"L2\":\n",
    "            self.base_loss = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\"base_loss must be 'L1' or 'L2'\")\n",
    "        self.penalty_weight = penalty_weight\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Base loss (e.g., MAE)\n",
    "        base = self.base_loss(y_pred, y_true)\n",
    "\n",
    "        # Compute mean of true labels in batch\n",
    "        # batch_mean = y_true.mean()\n",
    "        mean = labelsMean #batch_mean #0.5 #y_mean\n",
    "\n",
    "        # Penalize predictions close to mean\n",
    "        distance_from_mean = torch.abs(y_pred - mean)\n",
    "        penalty = 0.3 + torch.exp(-self.alpha * distance_from_mean).mean()\n",
    "\n",
    "        total_loss = base + self.penalty_weight * penalty**2\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22b9f3-393f-4454-ab44-7885e5d58ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding (sinusoidal version)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95230bc-2208-4719-82a5-fa5bbe129bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model for signal input\n",
    "class SignalTransformer(nn.Module):\n",
    "    def __init__(self, input_size=250, d_model=128, nhead=8, num_layers=4, dim_feedforward=256, num_classes=1):\n",
    "        super().__init__()\n",
    "        # print(\"Input size: \", input_size)\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        # self.pos_encoder = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True  # makes input shape (B, S, D)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # self.output_head_1 = nn.Linear(d_model, num_classes * 12)\n",
    "        # self.output_head_2 = nn.Linear(num_classes * 12, num_classes * 6)\n",
    "        # self.output_head_3 = nn.Linear(num_classes * 6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if x.ndim < 3:\n",
    "        #   return x\n",
    "        # x shape: (batch_size, seq_len, 250)\n",
    "        # print(\"Before Shape: \", x.shape)\n",
    "        # print(x.shape)\n",
    "        x = self.input_proj(x)               # (B, S, d_model)\n",
    "        # print(x.shape)\n",
    "        # x = self.pos_encoder(x)              # add positional encoding\n",
    "        out = self.transformer(x)              # (B, S, d_model)\n",
    "        # print(out.shape)\n",
    "        # print(\"Transformer Out: \", x)\n",
    "        # x = x.mean(dim=1)                    # mean pooling over sequence\n",
    "        # x = self.output_head_1(x)            # (B, num_classes)\n",
    "        # x = self.output_head_2(x)            # (B, num_classes)\n",
    "        # out = self.output_head_3(x)            # (B, num_classes)\n",
    "        # print(\"Out: \", out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac9605-2a76-40aa-87fd-a29f763d9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLightning(L.LightningModule):\n",
    "    def __init__(self, input_size = 250, d_model=128, num_heads=8, d_ff=512, num_classes = 1, num_layers=2, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            # nn.Linear(input_size, d_model),\n",
    "            nn.Sequential(\n",
    "                # SignalTransformer(input_size=input_size, d_model=input_size, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                SignalTransformer(input_size=input_size, d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                nn.ReLU(),          # <-- activation\n",
    "                # SignalTransformer(input_size=d_model, d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                # nn.ReLU(),          # <-- activation\n",
    "                # SignalTransformer(input_size=d_model, d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                # nn.ReLU(),          # <-- activation\n",
    "                # SignalTransformer(input_size=d_model, d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                # nn.ReLU(),          # <-- activation\n",
    "                # SignalTransformer(input_size=d_model, d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                # nn.ReLU(),          # <-- activation\n",
    "                # SignalTransformer(input_size=d_model, d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                # nn.ReLU(),          # <-- activation\n",
    "                # SignalTransformer(input_size=d_model, d_model=d_model, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                # nn.ReLU(),          # <-- activation\n",
    "                SignalTransformer(input_size=d_model, d_model=input_size, nhead=num_heads, dim_feedforward=d_ff, num_classes=num_classes),\n",
    "                nn.ReLU(),          # <-- activation\n",
    "            ),\n",
    "            #for _ in range(num_layers)\n",
    "            nn.LayerNorm(input_size),  # normalization layer\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, 128),\n",
    "                nn.Dropout(0.2),    # optional\n",
    "                # nn.Tanh(),          # <-- activation\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            ,\n",
    "            # nn.Sequential(\n",
    "            #     nn.Linear(256, 256),\n",
    "            #     nn.Dropout(0.2),    # optional\n",
    "            #     # nn.Tanh(),          # <-- activation\n",
    "            #     nn.ReLU(),\n",
    "            # ),\n",
    "            # nn.Sequential(\n",
    "            #     nn.Linear(256, 256),\n",
    "            #     nn.Dropout(0.2),    # optional\n",
    "            #     # nn.Tanh(),          # <-- activation\n",
    "            #     nn.ReLU(),\n",
    "            # ),\n",
    "            # nn.Sequential(\n",
    "            #     nn.Linear(256, 128),\n",
    "            #     nn.Dropout(0.2),    # optional\n",
    "            #     # nn.Tanh(),          # <-- activation\n",
    "            #     nn.ReLU(),\n",
    "            # ),\n",
    "            # nn.Sequential(\n",
    "            #     nn.Linear(128, 64),\n",
    "            #     nn.Dropout(0.2),    # optional\n",
    "            #     # nn.Tanh(),          # <-- activation\n",
    "            #     nn.ReLU(),\n",
    "            # ),\n",
    "            # nn.Sequential(\n",
    "            #     nn.Linear(64, 32),\n",
    "            #     nn.Dropout(0.2),    # optional\n",
    "            #     # nn.Tanh(),          # <-- activation\n",
    "            #     nn.ReLU(),\n",
    "            # ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(128, 32),\n",
    "                nn.Dropout(0.2),    # optional\n",
    "                # nn.Tanh(),          # <-- activation\n",
    "                nn.ReLU(),\n",
    "            ),\n",
    "            # nn.Linear(32, num_classes),\n",
    "            # nn.Softmax(),\n",
    "            # nn.Sigmoid(),\n",
    "            # nn.Tanh(),\n",
    "        ])\n",
    "        # self.output_head = nn.Linear(d_model, d_model)  # example output head\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        # self.loss_fn = MeanPenaltyLoss(penalty_weight=1)\n",
    "        # self.loss_fn = nn.MSELoss()\n",
    "        # self.loss_fn = nn.L1Loss()\n",
    "\n",
    "        self.negative_classifier = nn.Linear(32, num_classes)\n",
    "        self.positive_classifier = nn.Linear(32, num_classes)\n",
    "        \n",
    "        self.warmup_steps=200\n",
    "        self.total_steps=2000\n",
    "\n",
    "    def forward(self, x):\n",
    "        initialX = x\n",
    "        attenLayer = True\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            if attenLayer: # Skip connection\n",
    "                x = initialX + x\n",
    "                attenLayer = False\n",
    "        # return self.output_head(x)\n",
    "        # x = x.mean(dim=1)  \n",
    "        # positive = self.positive_classifier(x)\n",
    "        # negative = positive # self.negative_classifier(x)\n",
    "        return self.negative_classifier(x), self.positive_classifier(x) #  negative, positive\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        negative_labels = y[:, negativeIndex]   # shape: (batch_size, num_classes)\n",
    "        positive_labels = y[:, positiveIndex]   # shape: (batch_size, num_classes)\n",
    "        \n",
    "        negative_y_hat, positive_y_hat = self(x)\n",
    "        \n",
    "        # loss = self.loss_fn(y, y_hat)\n",
    "        # loss = self.loss_fn(positive_y_hat, positive_labels)\n",
    "        negative_loss = self.loss_fn(negative_y_hat, negative_labels)\n",
    "        positive_loss = self.loss_fn(positive_y_hat, positive_labels)\n",
    "        loss = negative_loss + positive_loss\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss #negative_loss #loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # return optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-2)\n",
    "        # simple linear warmup -> then cosine decay\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < self.warmup_steps:\n",
    "                return float(current_step) / max(1.0, float(self.warmup_steps))\n",
    "            # cosine after warmup\n",
    "            progress = float(current_step - self.warmup_steps) / max(1, float(self.total_steps - self.warmup_steps))\n",
    "            return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': LambdaLR(optimizer, lr_lambda),\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c7780-fb29-4424-a974-ae83e6eb5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "seq_len = 2000\n",
    "signal_dim = len(X_all[0])\n",
    "input_dim = len(X_all[0])\n",
    "d_model=128\n",
    "seed = 512\n",
    "num_classes = 3 # len(y[0])\n",
    "\n",
    "set_seed(seed)\n",
    "model = TransformerLightning(input_size=signal_dim, d_model=d_model, num_heads=4, d_ff=128, num_classes=num_classes, num_layers=4, lr=2e-3)\n",
    "# model = SignalTransformer(input_size=signal_dim, num_classes=5)\n",
    "# inputs = torch.randn(batch_size, seq_len, signal_dim)\n",
    "# outputs = model.forward(torch.stack([torch.tensor(X[0], dtype=torch.float32), torch.tensor(X[1], dtype=torch.float32)] ).cpu() )\n",
    "# print(len(y[0]))\n",
    "# print(\"Output: \", outputs, \". End\")  # torch.Size([16, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58ea87-2d1b-40a6-a30a-25e98eab9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = y_train.mean().item()\n",
    "y_std = y_train.std().item()\n",
    "print(\"y mean:\", y_mean)\n",
    "print(\"y std:\", y_std)\n",
    "print(\"min/max:\", y_train.min().item(), y_train.max().item())\n",
    "\n",
    "print(\"y mean:\", ((y_train - y_mean) / y_std).std())\n",
    "\n",
    "baseline = torch.mean(torch.abs(((y_train - y_mean) / y_std)))\n",
    "print(\"Baseline MAE (predict 0):\", baseline.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d0cdd-75c7-4d66-b670-69f5b9a972c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_grad = 0.0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        total_grad += p.grad.abs().sum().item()\n",
    "print(\"Total grad magnitude:\", total_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16554c85-f3c8-4805-aacd-84768520b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, lab in train_loader:\n",
    "    print(\"min y:\", lab.min(), \"max y:\", lab.max(), \"mean y:\", lab.mean())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07ade6-f383-4313-805b-eeb287cb7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = nn.Softmax()\n",
    "def evalPrecision(data_loader, debug = False, PUDevice = device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correctCount = [0, 0]\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            # Get labels\n",
    "            # print(\"Input Len: \", len(inputs))\n",
    "            # print(\"Labels Len: \", labels.shape)\n",
    "            inputs = inputs.to(PUDevice)\n",
    "            labels = labels.to(PUDevice)\n",
    "            \n",
    "            negative_labels = labels[:, negativeIndex]   # shape: (batch_size, num_classes)\n",
    "            positive_labels = labels[:, positiveIndex]   # shape: (batch_size, num_classes)\n",
    "            # print(\"Negative Labels Len: \", negative_labels.shape)\n",
    "            \n",
    "            negative_outputs, positive_outputs = model.forward( inputs )\n",
    "            # print(\"Output Len: \", positive_outputs.shape)\n",
    "            total += len(labels)\n",
    "            \n",
    "            negative_logitLabels = torch.argmax(negative_outputs, dim=1)\n",
    "            positive_logitLabels = torch.argmax(positive_outputs, dim=1)\n",
    "    \n",
    "            # print(\"logit argMax Len: \", negative_logitLabels.shape)\n",
    "            \n",
    "            \n",
    "            correctCount[negativeIndex] += (negative_logitLabels == negative_labels).sum().item()\n",
    "            correctCount[positiveIndex] += (positive_logitLabels == positive_labels).sum().item()\n",
    "            if debug:\n",
    "                print(\"N\", correctCount[negativeIndex], \" P\", correctCount[positiveIndex], \" out of\", total)\n",
    "            # print(labels.shape)\n",
    "            # print(outputs.shape)\n",
    "            # print(\"Label: \\n\", labels)\n",
    "            # print(\"Output: \\n\", outputs, \". End\")  # torch.Size([16, 5])\n",
    "            # break\n",
    "    if debug:\n",
    "        print(\"N\", correctCount[negativeIndex], \" P\", correctCount[positiveIndex], \"out of \", total)\n",
    "        print(\"N Precision\", correctCount[negativeIndex] / total, \" P Precision\", correctCount[positiveIndex] / total)\n",
    "    return np.array(correctCount) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edb037-5b79-4398-b978-86e6f5d274b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_progress = {\n",
    "    \"training\": [],\n",
    "    \"validation\": []\n",
    "}\n",
    "\n",
    "model_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe4420-a952-4c24-bdd8-b43a84e11e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the precision on the training data\n",
    "trainingPrecision = evalPrecision(train_loader, True, \"cpu\")\n",
    "model_progress[\"training\"].append(trainingPrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e64ca1-c392-4751-a103-dd31cd0c1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the precision on the validation data\n",
    "validationPrecision = evalPrecision(valid_loader, True, \"cpu\")\n",
    "model_progress[\"validation\"].append(validationPrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26eff55-acc2-49df-862f-89d3cb6a2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call it only after the training is finished\n",
    "evalPrecision(test_loader, True, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4653cd-5344-4cb1-b4a0-498a000b1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = nn.Softmax()\n",
    "model.eval()\n",
    "total = 0\n",
    "correctCount = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        # print(inputs)\n",
    "        outputs = model.forward( inputs.cpu() )\n",
    "        total += len(labels)\n",
    "        logitLabels = torch.argmax(outputs, dim=1)\n",
    "        correctCount += (logitLabels == labels).sum().item()\n",
    "        print(correctCount, \"out of \", total)\n",
    "        # print(labels.shape)\n",
    "        # print(outputs.shape)\n",
    "        print(\"Label: \\n\", labels)\n",
    "        print(\"logitLabels: \\n\", logitLabels, \". End\")  # torch.Size([16, 5])\n",
    "        print(\"Output: \\n\", outputs, \". End\")  # torch.Size([16, 5])\n",
    "        break\n",
    "print(correctCount / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb682d-7fe9-47d5-8d48-9b152a4aba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = nn.Softmax()\n",
    "model.eval()\n",
    "total = 0\n",
    "correctCount = 0\n",
    "confusionMatrix = [[0, 0], [0, 0]] # [lows [true lows, lows but predicted high], highs[true highs, highs but predicted lows]]\n",
    "cm_data = [[[], []], [[], []]] # negative [Actual, Predicted], positive [Actual, Predicted]\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs = inputs.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "        total += len(labels)\n",
    "        \n",
    "        negative_labels = labels[:, negativeIndex]   # shape: (batch_size, num_classes)\n",
    "        positive_labels = labels[:, positiveIndex]   # shape: (batch_size, num_classes)\n",
    "        # print(\"Negative Labels Len: \", negative_labels.shape)\n",
    "        labels = [[], []]\n",
    "        labels[negativeIndex] = negative_labels\n",
    "        labels[positiveIndex] = positive_labels\n",
    "        \n",
    "        negative_outputs, positive_outputs = model.forward( inputs )\n",
    "        # print(\"Output Len: \", positive_outputs.shape)\n",
    "        \n",
    "        negative_logitLabels = torch.argmax(negative_outputs, dim=1)\n",
    "        positive_logitLabels = torch.argmax(positive_outputs, dim=1)\n",
    "\n",
    "        logitLabels = [[], []]\n",
    "        logitLabels[negativeIndex] = negative_logitLabels\n",
    "        logitLabels[positiveIndex] = positive_logitLabels\n",
    "\n",
    "        # Previous code had an issue which reduced the size of the confusion matrix\n",
    "        # It has now been fixed\n",
    "        for n in range(len(labels[negativeIndex])):\n",
    "            cm_data[negativeIndex][0].append(labels[negativeIndex][n].item())\n",
    "            cm_data[negativeIndex][1].append(logitLabels[negativeIndex][n].item())\n",
    "            \n",
    "            cm_data[positiveIndex][0].append(labels[positiveIndex][n].item())\n",
    "            cm_data[positiveIndex][1].append(logitLabels[positiveIndex][n].item())\n",
    "            # confusionMatrix[labels[n].item()][logitLabels[negativeIndex][n]] += 1\n",
    "            # if labels[n].item() == logitLabels[n]:\n",
    "            #    confusionMatrix[labels[n].item()][logitLabels[n]] += 1\n",
    "            # else: # Predicted label x but actual label is y\n",
    "            #     confusionMatrix[labels[n].item()][1] += 1\n",
    "        # correctCount += (logitLabels == labels).sum().item()\n",
    "        # print(correctCount, \"out of \", total)\n",
    "        # print(labels.shape)\n",
    "        # print(outputs.shape)\n",
    "        # print(\"Negative Labels: \\n\", negative_labels)\n",
    "        # print(\"Negative Output: \\n\", negative_outputs, \". End\")  # torch.Size([16, 5])\n",
    "        # print(\"Positive Labels: \\n\", positive_labels)\n",
    "        # print(\"Positive Output: \\n\", positive_outputs, \". End\")  # torch.Size([16, 5])\n",
    "        # break\n",
    "# print(correctCount / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee68145-8643-460b-9fb9-b14a4e8a0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix\n",
    "negative_actual = cm_data[negativeIndex][0] #confusionMatrix[0]\n",
    "negative_predicted = cm_data[negativeIndex][1] #confusionMatrix[1]\n",
    "\n",
    "positive_actual = cm_data[positiveIndex][0] #confusionMatrix[0]\n",
    "positive_predicted = cm_data[positiveIndex][1] #confusionMatrix[1]\n",
    "# print(actual)\n",
    "# print(predicted)\n",
    "confusion_matrix = metrics.confusion_matrix(negative_actual, negative_predicted)\n",
    "print(confusion_matrix)\n",
    "\n",
    "matrix = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1, 2])\n",
    "\n",
    "matrix.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix Negative Affect 18\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(positive_actual, positive_predicted)\n",
    "print(confusion_matrix)\n",
    "\n",
    "matrix = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1, 2])\n",
    "\n",
    "matrix.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix Positive Affect 18\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a3fe1-ea59-458f-9a0f-aa78c48afb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Every20EpochsCallback(Callback):\n",
    "    def __init__(self, interval=20):\n",
    "        super().__init__()\n",
    "        self.interval = interval\n",
    "\n",
    "    # Called at end of each epoch\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "\n",
    "        if epoch >= self.interval and epoch % self.interval == 0:\n",
    "            train_precision = evalPrecision(train_loader, False, device)\n",
    "            valid_precision = evalPrecision(valid_loader, False, device)\n",
    "            model_progress[\"training\"].append(train_precision)\n",
    "            model_progress[\"validation\"].append(valid_precision)\n",
    "            print(\"Precision \", \"Train\", train_precision, \" Valid\", valid_precision)\n",
    "\n",
    "callback = Every20EpochsCallback(interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbf038-c63f-48ae-982f-98a795da4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce = nn.CrossEntropyLoss()\n",
    "# sm = torch.tensor([[0.5134, 0.4866], [0.5131, 0.4869]], dtype=torch.float)\n",
    "# ce(sm, torch.tensor([0, 1], dtype=torch.long))\n",
    "model_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903c047-b243-4137-bae7-7d514648f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = {\n",
    "    \"negative\": [],\n",
    "    \"positive\": [],\n",
    "}\n",
    "\n",
    "training = {\n",
    "    \"negative\": [],\n",
    "    \"positive\": [],\n",
    "}\n",
    "\n",
    "epochs = []\n",
    "\n",
    "for n in range(len(model_progress[\"training\"])):\n",
    "    if n % 26 == 0: # Some elements has been added in a wrong way every 25 times\n",
    "        continue\n",
    "    validation[\"negative\"].append(model_progress[\"validation\"][n][negativeIndex])\n",
    "    validation[\"positive\"].append(model_progress[\"validation\"][n][positiveIndex])\n",
    "\n",
    "    training[\"negative\"].append(model_progress[\"training\"][n][negativeIndex])\n",
    "    training[\"positive\"].append(model_progress[\"training\"][n][positiveIndex])\n",
    "    epochs.append(len(epochs) * 20)\n",
    "# model_progress[\"training\"][26+26]\n",
    "\n",
    "\n",
    "plt.plot(epochs, validation[\"negative\"], marker='x', label='Val Precision')\n",
    "plt.plot(epochs, training[\"negative\"], marker='o', label='Train Precision')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Training vs Validation Precision - Negative Affect18\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(epochs, validation[\"positive\"], marker='x', label='Val Precision')\n",
    "plt.plot(epochs, training[\"positive\"], marker='o', label='Train Precision')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Training vs Validation Precision - Positive Affect18\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7884ab-9438-4922-9a24-bc7590d24940",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=501,\n",
    "    accelerator=\"gpu\",   # or \"auto\" if you want Lightning to detect GPU\n",
    "    devices=1,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ffbd71-b3ba-4145-916e-43c17f40e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "air_quality = fetch_ucirepo(id=360)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "all_data = air_quality.data.features\n",
    "# y = air_quality.data.targets\n",
    "\n",
    "# # metadata\n",
    "# print(air_quality.metadata)\n",
    "\n",
    "# # variable information\n",
    "# print(air_quality.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a93eed-7ac3-4ec7-b9fa-4e794fd6b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "features = ['PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)',\n",
    "            'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)',\n",
    "            'PT08.S5(O3)', 'T', 'RH', 'AH']\n",
    "target = ['CO(GT)']\n",
    "\n",
    "X = all_data[features].values\n",
    "y = all_data[target].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220df122-a08c-4bde-bc36-cbbaeecb2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877fb74-7077-4978-83a3-caa9e6fd2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X = scaler_x.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y)\n",
    "input_dim = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572e6c4-1d14-4365-9c33-999325e1405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35902872-d363-4164-be97-6e267a95d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences (24-hour window)\n",
    "def create_sequences(X, y, seq_len=24):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])  # predict next hour\n",
    "    return torch.tensor(xs, dtype=torch.float32), torch.tensor(ys, dtype=torch.float32)\n",
    "\n",
    "x_seq, y_seq = create_sequences(X, y, input_dim)\n",
    "\n",
    "# DataLoader for Lightning\n",
    "train_loader = DataLoader(TensorDataset(x_seq, y_seq), batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337f7f1-9e36-4951-9417-00619920c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLightning(input_size=input_dim, d_model=64, num_heads=8, d_ff=64, num_classes=1, num_layers=1, lr=1e-3)\n",
    "signalModel = SignalTransformer(input_size=input_dim, d_model=64, nhead=8, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf430003-1139-4906-87ef-3de4a203cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor(X[0], dtype=torch.float32).reshape(1, 1, 12)\n",
    "print(X)\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5c1f9-c24a-4ade-a638-977a9524c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signalModel.eval()\n",
    "with torch.no_grad():\n",
    "  print(signalModel(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81431dff-c3ac-4751-a16c-acbd581b994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  print(model.forward(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f345c2f-2e12-4f27-9009-682da0f2d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=15,\n",
    "    accelerator=\"gpu\",   # or \"auto\" if you want Lightning to detect GPU\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6eebbf-10a7-46c7-9641-3b9104202a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=60,              # new total epochs\n",
    "    accelerator=\"gpu\",\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, ckpt_path=\"/content/lightning_logs/version_3/checkpoints/epoch=29-step=4410.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa578a-3beb-4874-b352-3df1749b8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
